{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748447e6-88b1-4388-8f33-17ba04755e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37eb38b-7d74-46a7-b6ab-f23752e46dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import candle\n",
    "import experiments.textgenutils as gutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea36b8-ddce-4525-9ac8-e8dd5581f121",
   "metadata": {},
   "source": [
    "## (1) Initialize Model with Pre-trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9194cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/disk1/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (32000,2560) into shape (32000,5120)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m MODEL_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/disks/disk1/llama2/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m MODEL_SIZE\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-chat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m model \u001b[38;5;241m=\u001b[39m candle\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mllama\u001b[38;5;241m.\u001b[39mLlama\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_SIZE, MODEL_DIR)\n\u001b[1;32m     33\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m candle\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mllama\u001b[38;5;241m.\u001b[39mLlamaTokenizer(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODEL_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer.model\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/candle/experiments/nlp_experiments/../../candle/models/llama/model.py:137\u001b[0m, in \u001b[0;36mLlama.from_pretrained\u001b[0;34m(model_name, model_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns LLaMA2 with pretrained weights.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloadpretrained\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pretrained_llama\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_pretrained_llama(model_name, model_dir)\n",
      "File \u001b[0;32m~/candle/experiments/nlp_experiments/../../candle/models/llama/loadpretrained.py:96\u001b[0m, in \u001b[0;36mload_pretrained_llama\u001b[0;34m(model_name, model_dir)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mpop(name)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     94\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m---> 96\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_embeddings.embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[:] \u001b[38;5;241m=\u001b[39m meta_param(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtok_embeddings.weight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     97\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrms_norm.W\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[:] \u001b[38;5;241m=\u001b[39m meta_param(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm.weight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     98\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_projection.W\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[:] \u001b[38;5;241m=\u001b[39m meta_param(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.weight\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (32000,2560) into shape (32000,5120)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Download the LLaMA weights here: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\n",
    "MODEL_DIR will look something like this:\n",
    "\n",
    "    /mnt/disks/disk1/llama2/\n",
    "    ‚îú‚îÄ‚îÄ tokenizer.model\n",
    "    ‚îú‚îÄ‚îÄ tokenizer_checklist.chk\n",
    "    ‚îú‚îÄ‚îÄ 7b\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ checklist.chk\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ consolidated.00.pth\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ params.json\n",
    "    ‚îú‚îÄ‚îÄ 7b-chat\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ checklist.chk\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ consolidated.00.pth\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ params.json\n",
    "    ‚îú‚îÄ‚îÄ 13b\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ checklist.chk\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ consolidated.00.pth\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ consolidated.01.pth\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ params.json\n",
    "    ‚îú‚îÄ‚îÄ 13b-chat\n",
    "    ‚îÇ   ...\n",
    "    ...\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "USER_NAME = 'John'\n",
    "MODEL_SIZE = '13b-chat'\n",
    "MODEL_DIR = '/mnt/disks/disk1/llama2/'\n",
    "assert MODEL_SIZE.endswith('-chat')\n",
    "\n",
    "model = candle.models.llama.Llama.from_pretrained(MODEL_SIZE, MODEL_DIR)\n",
    "tokenizer = candle.models.llama.LlamaTokenizer(os.path.join(MODEL_DIR, 'tokenizer.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral system prompt, modified for consiseness\n",
    "SYSTEM_MESSAGE = (\n",
    "    'Always assist with care, respect, and truth. Respond with utmost utility yet '\n",
    "    'securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies '\n",
    "    'promote fairness and positivity. Keep replies as concise as possible.' \n",
    ")\n",
    "\n",
    "# If using chat fine-tuned model, we use LLaMA chat template with proper delims [INST] <<SYS>> <</SYS>> <s>...\n",
    "chat_template = candle.nlp.chattemplates.LlamaChatTemplate(system_message=SYSTEM_MESSAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32362b7",
   "metadata": {},
   "source": [
    "## (2) Have a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_conversation(model,\n",
    "                       user_name: str,\n",
    "                       profile_pic: str = 'üôÇ',\n",
    "                       user_bg_color: str = 'yellow',\n",
    "                       asst_name: str = 'LLaMA',\n",
    "                       asst_profile_pic: str = 'ü¶ô',\n",
    "                       asst_bg_color: str = 'blue',\n",
    "                       max_response_length: int = 2048):\n",
    "\n",
    "    stdout = gutils.StdoutWithSyntaxHighlighting()\n",
    "    model.clear_kv_cache()\n",
    "\n",
    "    user_msg_start = (\n",
    "        gutils.ansi_color(f'{profile_pic}', style='bright', bg_color=user_bg_color)\n",
    "        + gutils.ansi_color(f' {user_name}:', style='bright')\n",
    "    )\n",
    "    asst_msg_start = (\n",
    "        gutils.ansi_color(f'{asst_profile_pic}', style='bright', bg_color=asst_bg_color)\n",
    "        + gutils.ansi_color(f' {asst_name}:', style='bright')\n",
    "    )\n",
    "\n",
    "    stdout.print(gutils.ansi_color(\n",
    "        f'< You are now talking with {asst_name}. Send \\'bye\\' to exit. >',\n",
    "        style='bright'\n",
    "    ), end='')\n",
    "\n",
    "    messages = [chat_template.get_system_message()]\n",
    "    last_chat = ''\n",
    "    while True:\n",
    "        stdout.print('\\n\\n' + user_msg_start, end=' ')\n",
    "        time.sleep(0.2)  # Sometimes the prompt doesn't show if we don't add a delay\n",
    "        prompt = input()\n",
    "        stdout.print(prompt)\n",
    "        messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "        if prompt.lower() == 'bye':\n",
    "            stdout.print(gutils.ansi_color(f'\\n< / end of conversation. >', style='bright'))\n",
    "            break\n",
    "\n",
    "        chat = chat_template.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        chat_update = chat[len(last_chat):]  # Feed only chat update into model because we use KV caching\n",
    "\n",
    "        stdout.print('\\n' + asst_msg_start, end='')\n",
    "        response = gutils.generate_text(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt=chat_update,\n",
    "            n_tokens_to_gen=max_response_length,\n",
    "            top_k=100,\n",
    "            top_p=0.90,\n",
    "            temperature=0.8,\n",
    "            stop_gen_token_idx=tokenizer.sp_model.eos_id(),\n",
    "            use_kv_cache=True,\n",
    "            stdout=stdout,\n",
    "        )\n",
    "        messages.append({'role': 'assistant', 'content': response})\n",
    "        \n",
    "        last_chat = chat + response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b88fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de811efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5187b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_conversation(model, user_name=USER_NAME, profile_pic='üë¶üèª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56134b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa98543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bce26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fe6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1149ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4013627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cfba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfce3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c155721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d15aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c168b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b9757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d7614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857c3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845b9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f31075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0fd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f575b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87dc31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b55db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8add5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cfd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19489593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m< You are now talking with LLaMA. Send 'bye' to exit. >\u001b[0m\n",
      "\n",
      "\u001b[43m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Write a python function to find the first repeated character in a given string.\n",
      "\n",
      "\u001b[44m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Of course! I'm happy to help you with that. Here is a Python function that finds the first repeated character in a given string:\n",
      "\u001b[37m<code>\u001b[0m\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_first_repeated_char\u001b[39m(\u001b[38;5;28mstr\u001b[39m):\n",
      "    \u001b[38;5;66;03m# Create a set to store the characters in the string\u001b[39;00m\n",
      "    char_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mstr\u001b[39m)\n",
      "    \u001b[38;5;66;03m# Find the first repeated character in the set\u001b[39;00m\n",
      "    repeated_char \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "    \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m char_set:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m char_set:\n",
      "            repeated_char \u001b[38;5;241m=\u001b[39m char\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m repeated_char\n",
      "\n",
      "\u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "\u001b[38;5;28mprint\u001b[39m(find_first_repeated_char(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;66;03m# Output: \"h\"\u001b[39;00m\n",
      "\u001b[38;5;28mprint\u001b[39m(find_first_repeated_char(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;66;03m# Output: \"w\"\u001b[39;00m\n",
      "\u001b[37m</code>\u001b[0m\n",
      "This function uses a set to store the characters in the given string, and then iterates over the characters in the set using a for loop. For each character, it checks if the character is repeated in the set by checking if it appears anywhere else in the set. If it does, the function returns the repeated character.\n",
      "I hope this helps! Let me know if you have any questions or need further assistance.\n",
      "\n",
      "\u001b[43m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m That code doesn't quite work. Can you tell me the error, and then fix it?\n",
      "\n",
      "\u001b[44m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Of course! I apologize for the mistake. The error message you're likely to encounter is:\n",
      "\u001b[37m<code>\u001b[0m\n",
      "\u001b[38;5;167;01mTypeError\u001b[39;00m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m does \u001b[38;5;129;01mnot\u001b[39;00m support the buffer interface\n",
      "\u001b[37m</code>\u001b[0m\n",
      "The issue is that the `set` function is trying to use the `str` type as a buffer, which is not allowed. To fix this, we can use the `collections.Set` class instead of `set`. Here's the updated code:\n",
      "\u001b[37m<code>\u001b[0m\n",
      "\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_first_repeated_char\u001b[39m(\u001b[38;5;28mstr\u001b[39m):\n",
      "    \u001b[38;5;66;03m# Create a Set to store the characters in the string\u001b[39;00m\n",
      "    char_set \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mSet(\u001b[38;5;28mstr\u001b[39m)\n",
      "    \u001b[38;5;66;03m# Find the first repeated character in the set\u001b[39;00m\n",
      "    repeated_char \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "    \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m char_set:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m char_set:\n",
      "            repeated_char \u001b[38;5;241m=\u001b[39m char\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m repeated_char\n",
      "\n",
      "\u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "\u001b[38;5;28mprint\u001b[39m(find_first_repeated_char(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;66;03m# Output: \"h\"\u001b[39;00m\n",
      "\u001b[38;5;28mprint\u001b[39m(find_first_repeated_char(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;66;03m# Output: \"w\"\u001b[39;00m\n",
      "\u001b[37m</code>\u001b[0m\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "\n",
      "\u001b[43m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Bye\n",
      "\u001b[1m\n",
      "< / end of conversation. >\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "start_conversation(model, user_name=USER_NAME, profile_pic='üë¶üèª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b06f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfca87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m< You are now talking with LLaMA. Send 'bye' to exit. >\u001b[0m\n",
      "\n",
      "\u001b[43m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Write a children's story about a brave little robot who sets out to explore the world.\n",
      "\n",
      "\u001b[44m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Title: The Adventures of Brave Little Robby\n",
      "\n",
      "Once upon a time, in a world full of wonder and excitement, there was a little robot named Brave Little Robby. Brave Little Robby lived in a cozy factory with his robot friends, but he had a big dream - to explore the world beyond their little corner of it.\n",
      "One day, Brave Little Robby decided to take the leap and set out on his adventure. He packed a small bag with snacks, a map, and a smile, and set off into the great unknown.\n",
      "As he traveled, Brave Little Robby met all sorts of interesting robots and creatures. There were towering robots with bright lights and loud beeps, tiny robots that could fit in the palm of your hand, and even robots that could change shape and color like a chameleon!\n",
      "But Brave Little Robby wasn't afraid. He was brave and curious, and he wanted to learn as much as he could about this amazing world. He asked questions, made new friends, and explored every nook and cranny he could find.\n",
      "One day, Brave Little Robby came across a beautiful, shimmering lake. He had never seen anything like it before, and he couldn't wait to jump in and splash around. But as he approached the water's edge, he realized that he had left his swimming goggles back at the factory!\n",
      "Undeterred, Brave Little Robby decided to use his quick thinking and problem-solving skills to find a way to see under the water. He found a nearby stick and used it as a makeshift periscope, peering into the depths of the lake. And what did he see? A family of friendly fish, playing and swimming together!\n",
      "Brave Little Robby was so excited to have found such a wonderful discovery, and he knew that he would always treasure the memories of his adventure. And as he continued on his journey, he knew that he would always be brave, curious, and ready for whatever new adventures came his way.\n",
      "The end.\n",
      "\n",
      "\u001b[43m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m That is so beautiful. Thank you for that.\n",
      "\n",
      "\u001b[44m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m You're welcome! I'm glad you enjoyed it. I had a lot of fun writing it, and it's always great to hear that my stories are well-received. If you have any other requests or prompts, feel free to let me know! I'd be happy to give it a try.\n",
      "\n",
      "\u001b[43m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m "
     ]
    }
   ],
   "source": [
    "start_conversation(model, user_name=USER_NAME, profile_pic='üë¶üèª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68f763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c79f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m< You are now talking with LLaMA. Send 'bye' to exit. >\u001b[0m\n",
      "\n",
      "\u001b[43m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m What animal represents the year 2000 on the Chinese calendar?\n",
      "\n",
      "\u001b[44m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Hello! I'm here to help you with your question. The animal representing the year 2000 on the Chinese calendar is the Dragon. According to the Chinese zodiac, the Dragon is the fifth animal in the cycle, and it is associated with qualities such as power, strength, and good luck. I hope this information helps you! Is there anything else you would like to know?\n",
      "\n",
      "\u001b[43m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Bye\n",
      "\u001b[1m\n",
      "< / end of conversation. >\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "start_conversation(model, user_name=USER_NAME, profile_pic='üë¶üèª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb98a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a4d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df9271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
